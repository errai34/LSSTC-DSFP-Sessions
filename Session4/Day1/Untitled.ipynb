{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python 3 code for computing posterior for Omega_m and h from JLA supernova data\n",
    "# For Python 2.7, print (x) should be replaced by print x\n",
    "\n",
    "# Alan Heavens 8 Septmeber 2016\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math as math\n",
    "%matplotlib inline\n",
    "\n",
    "# Number of MCMC samples and number of parameters:\n",
    "nsamples = 10000\n",
    "npars    = 2\n",
    "\n",
    "# Define (gaussian) width of the proposal distribution, one for each parameter:\n",
    "Sigma = [0.01,0.01]\n",
    "\n",
    "# Number of supernova bins:\n",
    "nSN = 31\n",
    "\n",
    "# Declare an empty array of the parameter values of each point. \n",
    "# Parameter 0 = Omega_m\n",
    "# Parameter 1 = h\n",
    "# Parameter npars will be used to store the ln(likelihood)\n",
    "Theta          = np.empty([nsamples,npars+1])\n",
    "\n",
    "# Utility vector to store mu(data)-mu(theory), temporarily:\n",
    "Dmu = np.empty(nSN)\n",
    "\n",
    "# Random starting point in parameter space; each parameter in range (0,1).\n",
    "# Set initial likelihood to low value so next point is accepted (could compute it instead):\n",
    "Theta[0,:] = [np.random.uniform(), np.random.uniform(), -1.e100]\n",
    "\n",
    "# Read in the Supernova data file:\n",
    "f    = open('jla_mub.txt', 'r') # 'r' = read only\n",
    "data = np.loadtxt(f)\n",
    "#delete(data,0,0) # Erases the first row (i.e. the header)\n",
    "f.close()\n",
    "\n",
    "# For convenience, define redshift bin (z) and ditance modulus (mu) arrays:\n",
    "z  = data[:,0]\n",
    "mu = data[:,1]\n",
    "\n",
    "# Plot the data:\n",
    "plt.plot(data[:,0],data[:,1],'o')\n",
    "npts    = 300\n",
    "z_plot  = np.linspace(0.01, 1.5, npts)\n",
    "mu_plot = np.empty(npts)\n",
    "         \n",
    "# Read in the covariance matrix (as a list), reshape it to 31 x 31, and then invert it:\n",
    "f   = open('jla_mub_covmatrix.txt', 'r') # 'r' = read only\n",
    "cov = np.loadtxt(f)\n",
    "cov = np.reshape(cov,(nSN,nSN))\n",
    "f.close()\n",
    "\n",
    "InvC = np.empty([nSN,nSN])\n",
    "InvC = np.linalg.inv(cov)\n",
    "\n",
    "# Define three functions to approximate the luminosity distance and distance modulus:\n",
    "# From Pen (1999).\n",
    "def eta( a, Omegam ):\n",
    "    \"Utility function used in DL\"\n",
    "    s = math.pow(1.0/Omegam-1.0,1.0/3.0)\n",
    "    etaval = 2.0 * math.sqrt(math.pow(s,3)+1.0) * math.pow(math.pow(a,-4)-\n",
    "                                                        0.1540*s*math.pow(a,-3)+\n",
    "                                                        0.4304*math.pow(s,2)*math.pow(a,-2)+\n",
    "                                                        0.19097*math.pow(s,3)/a+\n",
    "                                                        0.066941*math.pow(s,4),-0.125)   \n",
    "    return etaval\n",
    "\n",
    "def DLstar(z,Omegam):\n",
    "    \"Luminosity Distance at redshift z, in Mpc/h, for flat Universe with parameters Omegam, h\"\n",
    "    DLval = 3000.0*(1.0+z)*(eta(1.0,Omegam)-eta(1.0/(1.0+z), Omegam))\n",
    "    return DLval\n",
    "\n",
    "def mu_model(z,Omegam,h):\n",
    "    \"Distance modulus at redshift z, for flat Universe with parameters Omegam, h\"\n",
    "    mu_model = 25.0 - 5.0*math.log10(h) + 5.0*math.log10(DLstar(z,Omegam))\n",
    "    return mu_model\n",
    "\n",
    "# Define the likelihood function:\n",
    "\n",
    "def lnL(Omegam, h):\n",
    "    \"Ln of likelihood for flat Universe and SN distance modulus data\"\n",
    "\n",
    "# Treat unphysical regions by setting likelihood to (almost) zero:    \n",
    "    if(Omegam<=0 or h<=0):\n",
    "        lnL = -1.e100\n",
    "    else:\n",
    "            \n",
    "# Compute difference with theory mu at redshifts of the SN, for trial Omegam, h:\n",
    "        for j in range (nSN):\n",
    "            Dmu[j] = mu[j]-mu_model(z[j],Omegam,h)\n",
    "\n",
    "# Compute ln(likelihood) assuming gaussian errors (double sum done using vector/matrix ops):\n",
    "        lnL = -0.5*np.dot(Dmu,np.dot(InvC,Dmu))\n",
    "        \n",
    "    return lnL\n",
    "\n",
    "# Plot a few theoretical curves on top of the data:\n",
    "for i in range (npts):\n",
    "    mu_plot[i] = mu_model(z_plot[i], 0.3, 0.7)\n",
    "plt.plot(z_plot,mu_plot)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel(r'$\\mu$')\n",
    "\n",
    "for i in range (npts):\n",
    "    mu_plot[i] = mu_model(z_plot[i], 0.4, 0.5)\n",
    "plt.plot(z_plot,mu_plot)\n",
    "\n",
    "for i in range (npts):\n",
    "    mu_plot[i] = mu_model(z_plot[i], 0.6, 0.9)\n",
    "plt.plot(z_plot,mu_plot)\n",
    "plt.show()\n",
    "\n",
    "# Draw new proposed samples from a proposal distribution, centred on old values Omegam[i-1], h[i-1]\n",
    "# Accept or reject, and colour points according to ln(likelihood):\n",
    "\n",
    "# Compute initial likelihood value:\n",
    "Theta[0,npars] = lnL(Theta[0,0], Theta[0,1])\n",
    "    \n",
    "for i in range(1,nsamples):    \n",
    "    lnLPrevious = Theta[i-1,npars]\n",
    "    OmegamProp = np.random.normal(Theta[i-1,0],Sigma[0])\n",
    "    hProp      = np.random.normal(Theta[i-1,1],Sigma[1])\n",
    "    \n",
    "    lnLProp    = lnL(OmegamProp,hProp)\n",
    "\n",
    "# Metroplis-Hastings algorithm:\n",
    "\n",
    "    if(lnLProp > lnLPrevious):\n",
    "# Accept point if likelihood has gone up:\n",
    "        Theta[i,0]     = OmegamProp\n",
    "        Theta[i,1]     = hProp\n",
    "        Theta[i,npars] = lnLProp\n",
    "    else:\n",
    "# Otherwise accept it with probability given by ratio of likelihoods:\n",
    "        alpha = np.random.uniform()\n",
    "    \n",
    "#        print(lnLProp,lnLPrevious,alpha)\n",
    "        if(lnLProp - lnLPrevious > np.log(alpha)):\n",
    "            Theta[i,0]     = OmegamProp\n",
    "            Theta[i,1]     = hProp\n",
    "            Theta[i,npars] = lnLProp\n",
    "#            print('Accepted')\n",
    "        else:\n",
    "# Repeat the previous point in the chain:\n",
    "            Theta[i,0]     = Theta[i-1,0]\n",
    "            Theta[i,1]     = Theta[i-1,1]\n",
    "            Theta[i,npars] = lnLPrevious\n",
    "#            print('Rejected')\n",
    "   \n",
    "# Remove a burn in period, arbitrarily chosen to be the first 10% of the chain:\n",
    "nburn = math.floor(nsamples/10)\n",
    "    \n",
    "# Scatter plot of the samples:\n",
    "plt.scatter(Theta[nburn:,0], Theta[nburn:,1], c = -Theta[nburn:,npars])\n",
    "plt.xlim(0.15,0.45)\n",
    "plt.ylim(0.65,0.75)\n",
    "plt.xlabel(r'$\\Omega_m$')\n",
    "plt.ylabel('h')\n",
    "plt.show() \n",
    "\n",
    "# Histogram of Omegam:\n",
    "plt.hist(Theta[nburn:,0],bins=30)\n",
    "plt.xlabel(r'$\\Omega_m$')\n",
    "plt.show()\n",
    "\n",
    "# Histogram of h:\n",
    "plt.hist(Theta[nburn:,1],bins=30)\n",
    "plt.xlabel('h')\n",
    "plt.show()\n",
    "\n",
    "# Print mean of parameters, after removing a burn-in of 10% of samples:\n",
    "\n",
    "print ('Mean of Omegam = ',np.mean(Theta[nburn:nsamples,0]))\n",
    "print ('Mean of h      = ',np.mean(Theta[nburn:nsamples,1]))\n",
    "print ('Std of Omegam = ',np.std(Theta[nburn:nsamples,0]))\n",
    "print ('Std of h      = ',np.std(Theta[nburn:nsamples,1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
